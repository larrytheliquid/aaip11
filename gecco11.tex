\documentclass{acm_proc_article-sp}
\usepackage[mathletters]{ucs}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{tipa,textcomp}

\begin{document}

\title{[DRAFT] Verified Stack-Based Genetic Programming\\
via Dependent Types}

\subtitle{Genetic Programming}

\numberofauthors{1}
\author{
\alignauthor Larry Diehl\\
       \email{larrytheliquid@gmail.com}
}

\maketitle
\begin{abstract}
Genetic Programming (GP) can act as a powerful search tool for certain
classes of problems. Much research has been done exploring the
effectiveness of  various term representations, genetic operators, and
techniques for intelligently navigating the search space by taking
type information into account. This paper explores the less trodden
road of formally capturing invariants normally assumed in GP
implementations. Dependently Typed Programming (DTP) extends the
type-level expressiveness normally available in functional programming
languages to almost arbitrary propositions in formal systems. We use
DTP to express and enforce \textit{semantic} invariants relevant to GP
at the level of types, with a special focus on type-safe crossover for
strongly typed stack-based GP. Given the complexity involved in GP
implementations and the potential for bugs, we hope to help
researchers avoid errorneously attributing evolutionary explanations
to GP run phenomena by using a verified implementation. 
\end{abstract}

\category{D.1.1}{Programming Techniques}{Applicative (Functional) Programming}
\category{I.2.2}{Artificial Intelligence}{Automatic Programming}[program synthesis, program verification]

\terms{Languages, Reliability, Verification}

\keywords{stack-based gp, linear gp, dependent types, purely functional programming, formal methods}

\section{Introduction}

Genetic Programming (GP) as a field began by using the meta-language
(Lisp) to directly represent candidate solutions for a problem. This is primarily due
to the easy manipulation of parse trees and built-in evaluation
function over them. Naturally, a lot of GP's early and prominent work
was dominated by dynamic language implementations, clever avoidance of
type-issues during evolution, and tree-based program terms
for the algorithm to operate on.

Since then, there has been work on using type-awareness to inform the
different stages of the algorithm
\cite{montana:strongtree, tchernev:crossmethods}  
(population generation, genetic operators, etc.) There has also been
plenty of separate work on alternative representations (linear, graph,
grammar-based, etc.) 

Significant advances have been made investigating how variations of
the basic algorithm affect properties like search space, fitness
gradients, and overall performance. One area that has not seen as much
exploration is the correctness of the implementation of a GP
algorithm. GP involves significant complexity, as it must maintain a
population of program terms, update the population across generations
with term-manipulating genetic operators, evaluation of individuals,
selection, TODO.

The most frequently used method to compare the performance of a
particular GP implementation is to compute basic statistics about
things like fitness scores, size of terms, TODO and see how they
change as algorithm stages progress. This often takes the form of
looking at such tabulated, graphed, or otherwise visualized data and
drawing conclusions about how evolution must have done this or that to
explain interesting portions of the data. Seeing what evolution is
buying us is, after all, the point.

That leads us to a problem that we would like to address. Namely,
being sure that the behavior of the algorithm is what we expect. Given
the complexity of these algorithsm, it would be very unfortunate,
although not unimaginable, to have bugs in implementation that cause
certain phenomena to occur in the data analyzed in the end. We would
like to avoid researchers drawing conclusions from implementation
mistakes and attributing and propagating evolutionary explanations for
them.

In harmony with this goal is making sure evolution is used only where
necessary, rather than taking a swiss army knife approach of forcing
evolution solve all aspects of a problem. One
related GP problem is that of ill-formed programs. Typical
dynamic-type inspired solutions either \cite{koza:92}, assign a minimal fitness to
ill-formed programs, or treat failing operations as
NOOP's
\footnote{In a stack-based language, a runtime failure can act like a
  NOOP by continuing execution with an unmodified stack state. However,
  tree-based languages do not have a simple way to return nothing}.
The first approach does not scale up to problems involving more
complex types that are not related in meaningful ways for the purpose
of conversions. The second approach puts more of a burden on evolution, and the third
leads to bloat (a problem evolutionary algorithms already have enough
of.)

Instead, we will use static-typing inspired approaches featured in
strongly typed GP \cite{montana:strongtree, tchernev:forthcross} that make algorithmic
modifications to constrain the search space that evolution must
explore. Further, to fit with our other goal of avoiding
implementation mistakes, operating on a simple structure that still
has ample opportunity for useful program recombination is
desirable. Although tree structures are the most common GP term
representation, functions designed to manipulate them are not as
straightforward as those on list/linear structures. Tchernev
\cite{tchernev:forthcross, tchernev:crossmethods} has shown how simple
methods for type-safe linear crossover can be competitive with those
of subtree crossover. 1-dimensional flat structures such as lists are
intuitively simpler than nested trees, although more technically this
can be understood as operations on lists forming a monoid
\cite{spivey:catlists}. Additionally, functions operating on list monads have
several well-understood and mathematically-backed results in the
domain of permutations that are useful in a GP setting.

In essence, we would like to come up with a list of invariants that
the implementation of a GP algorithm should uphold in order to make us
more confident in explanations of emergent phenomena in terms of
evolution. A programming language that is capable of succinctly
expressing our desired properties, and has the semantics to be able to
check them, is desireable. For basic hygene, we want to use a purely
functional language (the most popular example being Haskell
\cite{spj:haskell}). The complexity of algorithm requires several
interacting pieces, decisions depending on randomness, the
manipulation of terms, and updating a population. An implementation in
an impure language leaves many an opportunity for mistakes in the
interaction of various side affecting operations. Conversely, a pure
settings allows for a more structured, modular approach, to GP that
explicitly represents side effects via monads. Simulation of the side
effects in question are already mature and well-understood.

Secondly, the typical GP program must deal with things like catching
exceptions produced by failing programs, and assigning them low
fitness. The random generation of programs can also produce programs
that fail to terminate or take a long time to terminate. The halting
problem tells us that we cannot distinguish between the
two, but it would useful to handle the two cases differently
(e.g. giving a low fitness to the former but allowing the latter to
complete running). Although though Haskell is a pure language, it is
also a partial language. When using a partial meta-language, it is
possible to conflate problems having to do with exceptions and
termination in the meta-language implementation, with those observed
in object-language. Consequently, we seek a language that is both pure
and total.

In a certain sense, Hindley-Milner based languages that are both pure
and total just shift errors from being classified as runtime errors to
being classified as logic errors. This is because part of totality is
for a function to be coverage complete in its case analysis. For
example, a function that is designed to only work on some cases of an
argument must wrap its output in a failure-simulating monad
(e.g. Maybe). Enough nested functions passing around such simulated
failure can lead to sloppy programming that fails too much in one
place, or returns nonsense but type-correct values in
others. Furthermore, we need to be able express more complex GP
domain-specific invariants (such as type-correct crossover) that are
not afforded to us by purity or totality alone. This finally leads us
to the class of languages suitable for the task at hand, namely
dependently typed programming languages.

In addition to being pure and total, languages with dependent types
are characterized by the property that labeled dependencies can exist
at the type level of programs. Moreover, the typical distinction
between the kinds of programs expressible at the level of values and
at the level of types is no longer. In other words, arbitrarily
complex functions can be used in type signatures in order to constrain
or otherwise express the kind of data that a function operates on.

\section{Stack Languages}

In stack-based languages \cite{kelly:forth} there is no
distinction between values and functions. Instead, each syntactic
element is referred to as a "word". Every word can be modeled as a
function which takes the previous stack state as a value and returns
the subsequent, possibly altered, state. Consider for example a
small word language in the boolean domain, made up of \texttt{true},
\texttt{false}, \texttt{and}, \texttt{or} , and \texttt{not}. A word
such as \texttt{true} that would typically be considered as a value
has no requirements on the input stack, and merely return the input
stack plus a boolean value of true pushed on top. On the other
hand, \texttt{and} requires the input stack to have at least two elements,
which is pops off in order to push their logical conjunction
back. It should be pointed out that unlike in tree-based grammars, a
previously pushed value may get used by an operation later in the
program. To reiterate, each word takes the entire global stack state
as input and returns the entire stack state as output. TODO

In a monotypic language, a simple static type system emerges which
assigns to each word a natural number representing the required input
stack length, and a second number representing the output stack
length. A sequence of such words forms a stack program, for which an
aggregate input/output pair can be computed. One can think of this
pair as a number representing how large of a stack is
\textit{consumed}, and another representing the size of the
\textit{produced} stack... or alternatively function arity which
indicates the change of a size-indexed global variable.

\section{Intro to Dependent Types}

For purposes of pedagogy, we will first consider how to represent a
population of terms/programs in a typical non-dependent functional
programming style. Thereafter, we will extend the example to use
dependent types.
\footnote{For a proper tutorial on dependently typed programming in Agda, see \cite{norell:agdatut}}

\subsection{Non-Dependent Types}

First let's create a new type representing the possible words for an
to be used for some evolutionary problem.

\begin{verbatim}
  data Word : Set where
    true false not and or : Word
\end{verbatim}

This simple example language is intended to operate on the boolean domain using
well-known constants and functions. Of course a stack program is not
merely a single word, but a sequence of them that we would like to
execute in order. The familiar cons-based list can serve as a
container for several words, so let us type it out.

\begin{verbatim}
  data List (A : Set) : Set where
    []  : List A
    _::_ : A → List A → List A

  Term : Set
  Term = List Word
\end{verbatim}

Notice in particular the \texttt{A : Set} part of the list
type. \texttt{Set} is the type of types in Agda, and the \texttt{A} is
a label that acts like a variable, but at the level of type
signatures. In other words, we have created a polymorphic list type
which is parameterized by what kind of data it can
contain. \texttt{Term} is a specific instantiation of lists that can
hold the \texttt{Word}s of our example language. Below are some
examples of programs we can now represent.

\begin{verbatim}
  identity : Term
  identity = not :: not :: []

  anotherTrue : Term
  anotherTrue = not :: not :: true :: []

  nand : Term
  nand = not :: and :: []
\end{verbatim}

GP requires us to work on not one but a collection of several terms,
referred to as the \textbf{population}. Normally, this might be
represented as a list of lists of terms.

\begin{verbatim}
  Population : Set
  Population = List Term
\end{verbatim}

While the type above is certainly functional, it leaves room for
error. This brings us to our first example of preserving some GP
invariant with the help of dependent types. Namely, the population
that GP acts upon is expected to be a certain size, and it should stay
that size as GP progresses from one generation to the next. Given the
sum complexity due to initialization, selection, and genetic
operators, it would not be surprising if a GP implementation had a bug
that caused the population to be bigger or smaller than expected
at some point in the run. Population size of course affects search
space, so conclusions drawn from GP run data that was affected by a
population size bug would be incorrect.

\subsection{Dependent Types}

In the dependently typed world, an easy and effective way to ensure
that some invariant is held is to create a type that can only possibly
construct values that satisfy said invariant. In our case we would
like the population size to be some exact natural number that we specify at the
beginning of the run. This brings us to one of the canonical examples
of a dependent type, the vector. We have already seen how the list
type takes a parameter to achieve polymorphism. Vectors take an
additional parameter representing their length.

\begin{verbatim}
  data Vec (A : Set) : ℕ → Set where
    []  : Vec A zero
    _::_ : {n : ℕ} → A → Vec A n →
      Vec A (suc n)

  Population : ℕ → Set
  Population n = Vec Term n
\end{verbatim}

The empty vector has a constant length of \texttt{zero}. The length of a vector
produced by cons is the \texttt{suc}cessor of whatever the length of
the tail is. Given such an inductive definition of a type, the natural
number index of any given vector can be nothing but its length. Just
like our definition of \texttt{Term}, \texttt{Population} is just a
specific instantiation of a more general type (\texttt{Vec}).

As an example, here is a small population of the three terms presented
earlier.

\begin{verbatim}
  pop : Population 3
  pop = identity :: anotherTrue :: nand :: []
\end{verbatim}

Once again, note that the type requires a population of exactly three
terms. If we were to supply any more or less, a type error would occur at
compile time. Effectively we move checking of certain \textit{semantic}
properties of our program to compile time, meaning much less can go
wrong once a program can be run.
\footnote{In fact, the only thing
  that can go wrong are logic errors due to bad encodings by the
  programmer. Typical runtime errors due to non-termination or lack of
  coverage are disallowed by the compiler.}

Now that we've seen how to construct a dependent type, let's see how
a function operating on \texttt{Vec} can make use of its properties.
During selection, GP will need to retrieve a candidate program from
the population. An all too common error taught even at introductory
level programming is indexing outside the bounds of a container
structure. How might one prevent this error? Ideally the type of the
parameter used to lookup a member should have exactly as many values
as the length of our vector. This way, a bijection would exist between
the lookup index type and the vector.

\begin{verbatim}
  data Fin : ℕ → Set where
    zero : {n : ℕ} → Fin (suc n)
    suc  : {n : ℕ} → Fin n → Fin (suc n)

  lookup : {A : Set} {n : ℕ} →
    Fin n → Vec A n → A
  lookup    zero (x :: xs) = x
  lookup (suc i) (x :: xs) = lookup i xs
\end{verbatim}

The type of finite sets \texttt{Fin} has exactly \texttt{n} possible
values for any \texttt{Fin n}. In the \texttt{lookup} function the
natural number index is shared between the finite set and vector
parameters. The effect of this sharing is that every finite set
argument has exactly as many possible constructions as the length of
the vector argument, statically preventing any index out of bounds
errors from occuring. Since our \texttt{Population} is merely a
specific kind of vector, we are able to use the safe \texttt{lookup}
when defining a function for the selection process.

\section{Requirement-Indexed Terms}

In the previous section we represented the terms in our population as
unadorened lists of words. In order to perform type-safe crossover in
a manner described by \cite{tchernev:forthcross}, the type of our
terms will need to be more telling.

\subsection{Informal Exploration}

Let us start by informally exploring what kind of input/output
requirements each word in our example language should express. 

\begin{verbatim}
  In  : Word → ℕ → ℕ
  In  true  n =     n
  In  false n =     n
  In  not   n = 1 + n
  In  and   n = 2 + n
  In  or    n = 2 + n

  Out : Word → ℕ → ℕ
  Out true  n = 1 + n
  Out false n = 1 + n
  Out not   n = 1 + n
  Out and   n = 1 + n
  Out or    n = 1 + n
\end{verbatim}

Both of these functions really just take a word, a natural number, and
return a natural number. There is nothing special about them that
enforces type constraints in a clever way. But, as we are being
informal let us pretend for a moment that two special properties exist
for these functions. The \texttt{In} function declares what the input
requirements are on the current stack state for each word.

Thus, the first property we will pretend to exist is that the natural number
parameter is a special implicit variable that unifies with whatever
other else we require in the return value. As such, \texttt{true} has
no input requirements whatsoever, and the stack state can be any
\texttt{n}. In contrast, \texttt{and} requires that the current stack
should at least be of size two, but the \texttt{n} is free to unify
with more inputs that \texttt{and} will ignore.

The second imagined property is that the \texttt{n} between the input
and output functions shall be one and the same. Hence, for
\texttt{true} the output stack shall be whatever the input stack was,
plus one value. Pay special attention to \texttt{and}, as it
illustrates how we expect operations to behave. We know that the input
must be \texttt{2 + n}, as \texttt{and} consumes two values. Of
course, \texttt{and} also produces one value to put back on the
stack, but not just any stack. It pushes it back on the same stack, so
between the output description of \texttt{1 + n} and the input
description of \texttt{2 + n}, we can see that the stack size will be
one less than whatever it was previously.

\subsection{Formal Terms}

Now that we know how we want the input and output requirements of
words to behave with respect to previous stack states, we can put our
prior work into practice by making a new concrete term type. When we
string together a sequence of words, the compound value should
indicate its input and output requirements. As such, our new
\texttt{Term} type should have one natural number index for input
requirements, and another for the resulting output.

\begin{verbatim}
  data Term (A : ℕ) : ℕ → Set where
    []  : Term A A

    _::_ : {n : ℕ} →
      (w : Word) → Term A (In w n) →
      Term A (Out w n)
\end{verbatim}

The empty term has no requirements on the input stack, and returns a
stack of the same size as the output, representing the identity
function. In the cons case the previous term has \texttt{In} as its
output and the resulting type has \texttt{Out} as its output. Notice
the use of an implicit \texttt{n} variable that is passed to both
\texttt{In} and \texttt{Out}. Agda will unify this variable to
whatever unique typing constraints make sense. In other words, our two
informal assumptions of a unifying \texttt{n} variable that is shared
between the input and output functions is formally captured in the
cons case of the term type.

\begin{verbatim}
  identity : Term 1 1
  identity = not :: not :: []

  anotherTrue : Term 0 1
  anotherTrue = not :: not :: true :: []

  nand : Term 2 1
  nand = not :: and :: []

  andAnd : Term 3 1
  andAnd = and :: and :: []
\end{verbatim}

Review the new types of the terms presented earlier, as well as a new
term. \texttt{andAnd} is an example of how \texttt{Term} correctly
composes input/output requirements. The first \texttt{and} requires
two arguments and produces one, which the second can use. Hence, the
compound term requires a total of three inputs and will produce one
output.

Notice that in this representation one syntatic representation of a
term can act as the value for many different types. Specifically, what
can change is the original number of arguments on the stack.

\begin{verbatim}
  bc : Term 2 1
  bc = and :: []

  ab : Term 3 2
  ab = and :: []
\end{verbatim}

\section{Total Evaluation}

When comparing relative performance between evolved terms, one needs
to evaluate them against some fitness function. We will proceed to write
an evaluation function for the example language we have used so far.

In Agda any function the user writes must be total. For Agda's
purposes, totality constitutes passing termination and coverage checks
at compile time. Termination checking ensures that recursive calls are
made with data that structurally decreases. Coverage checking ensures
that a function handles every valid case for each of its arguments.

\begin{verbatim}
  eval : {A C : ℕ} → Term A C → Vec Bool A →
    Vec Bool C
  eval [] as = as
  eval (true :: xs) as = true :: eval xs as
  eval (false :: xs) as = false :: eval xs as
  eval (not :: xs) as with eval xs as
  ... | c :: cs = ¬ c :: cs
  eval (and :: xs) as with eval xs as
  ... | c₂ :: c₁ :: cs = (c₁ ∧ c₂) :: cs
  eval (or :: xs) as with eval xs as
  ... | c₂ :: c₁ :: cs = (c₁ ∨ c₂) :: cs
\end{verbatim}

In addition to the term to evaluate, \texttt{eval} takes a vector of
booleans \footnote{Do not get confused by the true/false constructors
  of the \texttt{Bool} type and \texttt{Term} types. Agda can
  differentiate between overloaded constructor names according to the
  type they refer to in context.}
whose length \texttt{A} is equal to the number of inputs the
term expects. The return type of the function is another vector of
bools \texttt{C}, matching the evaluated term's output. Both of these
properties of course enforced statically, giving more assurance that
are algorithm is doing what we expect.

\section{Non-Trivial Languages}

The representation used can be extended to more complex languages,
including operations that use the entire current stack as the input
rather than just one or two top elements.

\begin{verbatim}
  data Word : Set where
    dup swap pop square : Word

  In : Word → ℕ → ℕ
  In dup    n = 1 + n
  In swap   n = 2 + n
  In pop    n = 1 + n
  In square n =     n

  Out : Word → ℕ → ℕ
  Out dup    n = 2 + n
  Out swap   n = 2 + n
  Out pop    n =     n
  Out square n = n * n
\end{verbatim}

Operation \texttt{dup} requires a top element to be present and
duplicates it, \texttt{swap} requires two elements to be present and
pushes them back on in swapped order (keeping the net stack size the
same), and \texttt{pop} requires one element to be on the stack and
removes it. Those three should be familiar to programmers of stack
languages, but \texttt{square} is a bit more interesting. It requires
any number of elements to be on the stack. It proceeds to pop all of
them, then pushes enough copies of the popped stack to equal the
square of its original size. This manifests itself as the value
\texttt{n * n} for the \texttt{Out} case of \texttt{square}.

\begin{verbatim}
  eval : {A C : ℕ} → Term A C → Vec Bool A →
    Vec Bool C
  eval (dup :: xs) as with eval xs as
  ... | c :: cs = c :: c :: cs
  eval (swap :: xs) as with eval xs as
  ... | c₂ :: c₁ :: cs = c₁ :: c₂ :: cs
  eval (pop :: xs) as with eval xs as
  ... | c :: cs = cs
  eval (_::_ {n = n} square xs) as with eval xs as
  ... | cs = concat (replicate {n = n} cs)
\end{verbatim}

The only case here worth drawing attention to is that of
\texttt{square}, which \texttt{replicate}s the output stack n times
and \texttt{concat}s into a single term.
\footnote{For dependently typed programmers, it should be pointed out
  that our representation does not run into termination issues when
  defining semantics for operations that increase the input stack. }
We are assured that even in the presence of somewhat complex semantics
for terms that the resulting arity must match the input/output
requirements declared by \texttt{In}/\texttt{Out}.

\section{Transitive Operations}

When writing genetic operators, for example
\cite{tchernev:forthcross}'s 1-point crossover, we need to take
subsections of different terms and recombine them in a safe
matter. Tchernev points out that we need to split parent terms at a
point of equal output stacks in order for a safe recombination. But,
what is the criterion for a safe append of two arbitrary terms after
they have been split in this manner?

\subsection{Transitive Append}

As was hinted at in the variable names of the example at the end of
section \textbf{Requirement-Indexed Terms}, a safe append illustrates
the transitive property. 

\begin{verbatim}
  _++_ : {A B C : ℕ} →
    Term B C → Term A B →
    Term A C
  [] ++ ys = ys
  (x :: xs) ++ ys = x :: (xs ++ ys)

  abc : Term 3 1
  abc = bc ++ ab
\end{verbatim}

If an attempt is made to append two terms whose inputs/output
requirements do not satisfy each other, a compile error will
occur. Using a function with a such an informative type gives a high
degree of confidence that we are doing the right thing when used
inside another function such as crossover. In fact, as we shall soon
see the type of this function gives us more than just confidence.

\subsection{Transitive Split}

Now that we have a function to safely recombine terms in a transitive
way, we need to come up with a compatible way to split a crossover
parent. The following is a derivative of the \texttt{SplitView} type
in \cite{oury:tpop}.

\begin{verbatim}
  data Split {A C : ℕ} (B : ℕ) :
    Term A C → Set where
    _++'_ :
      (xs : Term B C)
      (ys : Term A B) →
      Split B (xs ++ ys)
\end{verbatim}

The type above captures exactly how we would like split terms to be
represented such that they can be transitively recombined. The
\texttt{B} natural number index reveals the satisfied
input/output point a term was split at, and the term index is the
value we are splitting. The constructor carries the two
subterms which share \texttt{B} in a way that the resulting type can
recombine the two via \texttt{xs ++ ys}.

Given two parent terms split in such a way, crossover needs to produce
two offspring that swap the subterms in the splits. Functions for both
of these swaps can be straightforwardly defined.

\begin{verbatim}
  swap₁ : {A B C : ℕ} {xs ys : Term A C} →
    Split B xs → Split B ys → Term A C
  swap₁ (xs ++' ys) (as ++' bs) = xs ++ bs

  swap₂ : {A B C : ℕ} {xs ys : Term A C} →
    Split B xs → Split B ys → Term A C
  swap₂ (xs ++' ys) (as ++' bs) = as ++ ys
\end{verbatim}

\subsubsection{Dependent Pairs}

Given some term and a natural number, we would like to split the term
at the point indexed into. This function will be the key component of
determining the split in the female parent of
crossover. \texttt{Split} is specific enough to tell us the shared
\texttt{B} between the two subterms. However, for the purposes of this
function we do not care what \texttt{B} is (actually, we would like
the function to tell us what it is.)

\begin{verbatim}
  data Σ (A : Set) (B : A → Set) : Set where
    _,_ : (x : A) → B x → Σ A B
\end{verbatim}

A non-dependent pair, or tuple, carries 2 values of arbitrary
types. In the dependent version of pairs, the \textit{value} in the first
component is used to determine the \textit{type} in the second
component. One dependently typed programming technique is using a
dependent pair to hide the index type of a return value when you don't
know or care what it will be. For example, sometimes we would merely
like to write down a vector value and have the compiler figure out the
unique possible length.

\begin{verbatim}
  specifiedLength : Σ ℕ (λ n → Vec Bool n)
  specifiedLength = 3 , true :: false :: true :: []

  discoveredLength : Σ ℕ (λ n → Vec Bool n)
  discoveredLength = _ , true :: false :: true :: []
\end{verbatim}

Note the use of an anonymous function in the type. Remember that in DTP
we can do anything at the type level that we can do at the value
level, including the use of our ultimate mascot $\lambda$. With this
dependent pair trick up our sleeves, we can define \texttt{split}.

\begin{verbatim}
  split : {A C : ℕ} (n : ℕ) (xs : Term A C) →
    Σ ℕ (λ B → Split B xs)
  split   zero  xs = _ , [] ++' xs
  split (suc n) [] = _ , [] ++' []
  split (suc n) (x :: xs) with split n xs
  split (suc A) (x :: ._) | _ , xs ++' ys =
    _ , (x :: xs) ++' ys
\end{verbatim}

\section{Type-Preserving Crossover}

With the types and functions already defined, being able to define a
crossover function that takes two parent terms of the same type and
returns two child terms of the same type is not far away.

\subsection{Split Female}

For the first step in 1-point crossover we need to split the first
parent (referred to here as the female) at some random point. Thus, we
need to know the length of the female, then choose a random number
bounded by that length.

\begin{verbatim}
  length : {A C : ℕ} → Term A C → ℕ
  length [] = 0
  length (x :: xs) = suc (length xs)

  splitFemale : {A C : ℕ} → (xs : Term A C) → ℕ →
    Σ ℕ (λ B → Split B xs)
  splitFemale xs rand with rand mod (suc (length xs))
  ... | i = split (toℕ i) xs
\end{verbatim}

Note that we use a \texttt{\_mod\_} function which returns a finite set
representing the modulus of its two arguments. The definition of this
function can be found in the supplementary source code, as it is not
particularly relevant to what we would like to explain.

\subsection{Split Male}

Based upon the \texttt{B} index at which the female was split, the
male split can be determined by choosing a random member of all
possible compatible splits.

\begin{verbatim}
  splits : {A C : ℕ} (n B : ℕ) (xs : Term A C) →
    Σ ℕ (λ n → Vec (Split B xs) n)
  splits zero B xs with split zero xs
  ... | B' , ys with B =? B'
  ... | yes p rewrite p = _ , ys :: []
  ... | no p = _ , []
  splits (suc n) B xs with split (suc n) xs
  ... | B' , ys with B =? B' | splits n B xs
  ... | yes p | _ , yss rewrite p = _ , ys :: yss
  ... | no p | _ , yss = _ , yss
\end{verbatim}

\subsubsection{Propositional Equality}

In the definition of \texttt{splits} we simultaneously split at all
possible positions of the male term, and filter out ones that would
not be compatible with respect to transitively appending them back
together.

It is intuitive that in the algorithm we must compare the \texttt{B}
that we are splitting on and the \texttt{B'} in the current
split. Normally a comparison of two terms is performed by passing them
to a function that returns a boolean value, then doing something
different for the true and false cases. While a boolean value is
enough to convince a certain kind of person that action should be
taken, it is not enough to convince a type system.

Consider the \texttt{yes p} case (analogous to a typical \texttt{true}
case) within the \texttt{splits zero} case. We would like to return
our freshly split \texttt{ys} value, but the type checker won't have
it. Why not? If we look at the type signature of \texttt{splits}, it
requires a \texttt{Split B xs}, but \texttt{ys} is a \texttt{Split B'
  xs}. Luckily the \texttt{=?} comparison function returned something
more than just a boolean, but a constructive proof that both compared
values were in fact the same. We pass the proof \texttt{p} (pattern
matched as \texttt{yes p}) to Agda's \texttt{rewrite} keyword to
convince the type checker that \texttt{ys : Split B' xs} is acceptable
because \texttt{B ≡ B'}.

What should you take away from all this? Mainly that the type checker
needs to be convinced with formal constructive evidence to enforce
invariants you prescribe. This evidence is in practice easy to work
with, as it is made of ordinary dependent types like everything
else. The payoff is that confidence that the burden of verifying that
a program behaves as it is expected to is lifted from the programmer's
shoulders and onto the type checker's.

\subsubsection{Defining Crossover}

When we split the male parent we choose a random member of the
type-correct splits. However, this function returns a value of type
\texttt{Maybe}, so that it may return \texttt{nothing} if there is no
compatible split at all.

\begin{verbatim}
  splitMale : {A C : ℕ} (xs : Term A C) →
    (B rand : ℕ) → Maybe (Split B xs)
  splitMale xs B rand
    with splits (length xs) B xs
  ... | zero , [] = nothing
  ... | suc n , xss
    = just (lookup (rand mod suc n) xss)
\end{verbatim}

Note that the proof complexity in the implementation of
\texttt{splits} is isolated. Once we have a function definition that
typechecks, we can freely use it without having to do any work over
again.

Finally, we can write \texttt{crossover} to combine the female and
male splits, and return both children using the \texttt{swap}s defined
earlier.

\begin{verbatim}
  crossover : {A C : ℕ}
    (female male : Term A C) (randF randM : ℕ) →
    Term A C × Term A C
  crossover female male randF randM
    with splitFemale female randF
  ... | B , xs with splitMale male B randM
  ... | nothing = female , male
  ... | just ys = swap₁ xs ys , swap₂ xs ys
\end{verbatim}

In the case where no valid male swap existed, we return the original
two parents.

\section{Conclusion}

\bibliographystyle{abbrv}
\bibliography{gecco11}

\end{document}
